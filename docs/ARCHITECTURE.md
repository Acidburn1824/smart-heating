# üß† Smart Heating - Architecture

## Vision

Custom component HACS pour Home Assistant qui ajoute une anticipation intelligente
du chauffage bas√©e sur l'apprentissage de l'inertie thermique et l'IA.

Compatible avec Versatile Thermostat, les entit√©s climate natives, ou tout thermostat HA.

## Structure du projet

```
custom_components/smart_heating/
‚îú‚îÄ‚îÄ __init__.py              # Setup int√©gration + platforms
‚îú‚îÄ‚îÄ manifest.json            # Metadata HACS
‚îú‚îÄ‚îÄ config_flow.py           # Configuration UI (flux √©tape par √©tape)
‚îú‚îÄ‚îÄ const.py                 # Constantes
‚îú‚îÄ‚îÄ strings.json             # Traductions EN
‚îú‚îÄ‚îÄ translations/
‚îÇ   ‚îî‚îÄ‚îÄ fr.json              # Traductions FR
‚îÇ
‚îú‚îÄ‚îÄ coordinator.py           # DataUpdateCoordinator - cerveau principal
‚îú‚îÄ‚îÄ thermal_model.py         # Mod√®le d'inertie thermique (apprentissage)
‚îú‚îÄ‚îÄ anticipation.py          # Moteur d'anticipation (quand d√©marrer)
‚îú‚îÄ‚îÄ schedule_parser.py       # Lecture des schedules / presets
‚îÇ
‚îú‚îÄ‚îÄ llm/                     # Providers LLM modulaires
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Factory + base class
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # Classe abstraite LLMProvider
‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py   # OpenAI (GPT-4o-mini, GPT-4o, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ anthropic_provider.py # Claude (claude-sonnet, opus, haiku)
‚îÇ   ‚îú‚îÄ‚îÄ ollama_provider.py   # Ollama (local: llama3, mistral, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ ha_conversation.py   # Via l'int√©gration HA Conversation (Google, OpenAI, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ none_provider.py     # Mode sans IA (algorithme pur)
‚îÇ
‚îú‚îÄ‚îÄ sensor.py                # Sensors expos√©s dans HA
‚îú‚îÄ‚îÄ climate.py               # (optionnel) climate wrapper avec anticipation
‚îú‚îÄ‚îÄ number.py                # Input numbers (marge, anti-cycle, etc.)
‚îú‚îÄ‚îÄ switch.py                # Switches (activer/d√©sactiver anticipation)
‚îú‚îÄ‚îÄ binary_sensor.py         # Binary sensors (anticipation active, anti-cycle)
‚îú‚îÄ‚îÄ diagnostics.py           # Diagnostics pour debug
‚îî‚îÄ‚îÄ services.yaml            # Actions/services custom
```

## Flux de donn√©es

```
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ   Schedule    ‚îÇ (sensor, scheduler, preset change)
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Temp Room   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    SmartHeating        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Temp Ext    ‚îÇ
‚îÇ  (sensor)    ‚îÇ    ‚îÇ    Coordinator         ‚îÇ    ‚îÇ  (sensor)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ                       ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                    ‚îÇ  ‚îÇ Thermal Model   ‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  ‚îÇ (inertie)       ‚îÇ  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Weather     ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ  (forecast)  ‚îÇ
                    ‚îÇ                       ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                    ‚îÇ  ‚îÇ Anticipation    ‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  ‚îÇ Engine          ‚îÇ  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Climate      ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ  (VTherm/     ‚îÇ
                    ‚îÇ                       ‚îÇ    ‚îÇ   Netatmo/    ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îÇ   any)        ‚îÇ
                    ‚îÇ  ‚îÇ LLM Provider    ‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ  ‚îÇ (optionnel)     ‚îÇ  ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Providers LLM

### Interface commune (base.py)

```python
class LLMProvider(ABC):
    @abstractmethod
    async def async_get_adjustment(
        self,
        zone_name: str,
        thermal_data: dict,      # inertie, sessions, vitesses
        weather_forecast: dict,   # pr√©visions m√©t√©o
        current_state: dict,      # temp int, ext, consigne
        context: str,             # "morning" | "evening"
    ) -> LLMResponse:
        """Retourne un ajustement de marge + conseil texte."""
```

### LLMResponse

```python
@dataclass
class LLMResponse:
    margin_adjustment: float   # ex: 0.05 = +5% de marge
    confidence: float          # 0.0 - 1.0
    reasoning: str             # explication courte
    raw_response: str          # r√©ponse brute pour debug
```

### Providers disponibles

| Provider | Config | Co√ªt | Latence | Qualit√© |
|----------|--------|------|---------|---------|
| **None** | Rien | 0‚Ç¨ | 0ms | Algo pur, pas d'IA |
| **Ollama** | URL serveur | 0‚Ç¨ | 1-5s | Bon avec llama3/mistral |
| **OpenAI** | Cl√© API | ~0.01‚Ç¨/j | 1-3s | Excellent |
| **Anthropic** | Cl√© API | ~0.01‚Ç¨/j | 1-3s | Excellent |
| **HA Conversation** | Int√©gration HA | D√©pend | Variable | D√©pend du backend |

## Config Flow UI

### √âtape 1 : Zone
- Nom de la zone
- Capteur temp√©rature int√©rieure
- Capteur temp√©rature ext√©rieure
- Entit√© climate (VTherm ou autre)

### √âtape 2 : Schedule
- Source de consigne (sensor schedule / presets VTherm / manual)
- Entit√© weather (optionnel)

### √âtape 3 : Param√®tres
- Marge de s√©curit√© (default 15%)
- Warmup ignore minutes (po√™le: 12, radiateur: 0)
- Anti short cycle (oui/non + dur√©e)
- Minimum sessions avant anticipation (default 3)

### √âtape 4 : IA (optionnel)
- Provider : None / OpenAI / Anthropic / Ollama / HA Conversation
- Selon le provider:
  - OpenAI: cl√© API + mod√®le (gpt-4o-mini, gpt-4o)
  - Anthropic: cl√© API + mod√®le (claude-sonnet-4-5-20250514, etc.)
  - Ollama: URL + mod√®le (llama3, mistral, etc.)
  - HA Conversation: s√©lection de l'agent configur√©
- Fr√©quence appels (1x/jour, 2x/jour)
- Heure(s) d'appel

## Sensors cr√©√©s par zone

| Sensor | Description |
|--------|-------------|
| `sensor.smart_heating_{zone}_state` | √âtat (learning/ready/anticipating/idle) |
| `sensor.smart_heating_{zone}_sessions` | Nombre de sessions collect√©es |
| `sensor.smart_heating_{zone}_min_per_deg` | Minutes par degr√© |
| `sensor.smart_heating_{zone}_speed` | Vitesse mont√©e ¬∞C/min |
| `sensor.smart_heating_{zone}_optimal_start` | Heure de d√©marrage optimale |
| `sensor.smart_heating_{zone}_anticipation` | Minutes d'anticipation calcul√©es |
| `sensor.smart_heating_{zone}_llm_advice` | Dernier conseil IA |
| `binary_sensor.smart_heating_{zone}_anticipating` | Anticipation en cours |
| `binary_sensor.smart_heating_{zone}_anti_cycle` | Anti-cycle actif |
| `number.smart_heating_{zone}_margin` | Marge de s√©curit√© ajustable |
| `switch.smart_heating_{zone}_enabled` | Activer/d√©sactiver |

## Stockage des donn√©es

Fichier JSON par zone dans `/config/.storage/smart_heating_{zone}.json`
- Sessions de chauffe (max 100, FIFO)
- Mod√®le d'inertie calcul√©
- Historique LLM (7 derniers jours)
- Param√®tres calibr√©s
